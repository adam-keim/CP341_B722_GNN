{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9843fb-7325-45c9-9623-1e3d23778a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ce3a4-14b7-4d4a-856b-e386e976f39d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c180ee4-ec15-4fdb-aa8a-2cf5009e6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FpsDataset(Dataset):\n",
    "        \n",
    "    def __init__(self, fingerprints, labels):\n",
    "        pca = PCA(n_components=32) # reduce data set vector length to 32\n",
    "        pca_fps = pca.fit_transform(fingerprints)\n",
    "        self.data_tens = torch.from_numpy(pca_fps)\n",
    "        self.label_tens = torch.from_numpy(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.label_tens.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_tens[idx], self.label_tens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65fc47-5d03-4fce-b7e5-fb3b278b3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_fps_data(path='fps_and_labels.pickle'):\n",
    "    with open('fps_and_labels.pickle', 'rb') as f:\n",
    "        fingerprints, labels = pickle.load(f)\n",
    "    return (fingerprints, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa5e46-143a-4115-8a24-aa1a65794702",
   "metadata": {},
   "source": [
    "# NN Implementation\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c58dc-ed7c-46ca-96de-735553faea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Data, Error func, optimizer, NN classifier\n",
    "# 1. Data\n",
    "fingerprints, labels = unpickle_fps_data() \n",
    "dataset = FpsDataset(fingerprints, labels) # initialize dataset obj\n",
    "# creates a dataloader to batch data\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=256, shuffle=True)\n",
    "# 2. Error\n",
    "err_func = torch.nn.MSELoss()\n",
    "# 3. Classifier\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(32,16),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(16,1)  \n",
    ")\n",
    "# 4. Optimizer\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0770cc-6617-4a3f-a7ba-22cd3c957001",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fb716-8fc3-4800-a5e5-b00248c57c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Training loop:\n",
    "for i in range(epochs):\n",
    "    # loop through training batches\n",
    "    for idx, (data, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = torch.sigmoid(classifier(data))\n",
    "        error = err_func(predictions, y.reshape((y.size(0),1))) \n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "    # track error over whole dataset per epoch\n",
    "    predictions = torch.sigmoid(classifier(dataset.training_tens))\n",
    "    loss_list.append(err_func(predictions, dataset.tr_label_tens.reshape(\n",
    "        dataset.tr_label_tens.size(0),1)).detach().numpy())\n",
    "    # track error over tests per epoch\n",
    "    predictions = torch.sigmoid(classifier(dataset.test_tens))\n",
    "    test_predictions = predictions.reshape(-1).detach().numpy().astype(int)\n",
    "    test_labels = dataset.te_label_tens.detach().numpy()\n",
    "    total_correct = 0\n",
    "    for i in range(781):\n",
    "        if test_predictions[i] == test_labels[i]:\n",
    "            total_correct += 1\n",
    "    accuracy_list.append(total_correct/7.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0020b97-50bf-4132-90df-b96c2a490be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(clss, fps_dataset):\n",
    "    predictions = torch.sigmoid(clss(fps_dataset.test_tens))\n",
    "    test_predictions = predictions.reshape(-1).detach().numpy().astype(int)\n",
    "    test_labels = fps_dataset.te_label_tens.detach().numpy()\n",
    "    total_correct = 0\n",
    "    for i in range(781):\n",
    "        if test_predictions[i] == test_labels[i]:\n",
    "            total_correct += 1\n",
    "    return total_correct/7.81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6791c3-1d6d-4ba1-9503-0df6e84c1d63",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cdfb7f4-f283-45e4-a6e3-a90fe928f28c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (4017072579.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"Final Accuracy: %f\" % get_accuracy(classifier, dataset)\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Accuracy: %f\" % get_accuracy(classifier, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974acf24-eb2a-484d-bedd-18351757488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, epochs+1)), loss_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2008a0-300c-4c53-ae62-e1126dd687ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, epochs+1)), accuracy_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51a5fb-58b7-4e01-b5bb-e4b7e1282ebd",
   "metadata": {},
   "source": [
    "# Classifier Hyperparam Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f51a8e-da1f-45e8-9149-f9815a47ce20",
   "metadata": {},
   "source": [
    "### Classifier Testing Helper Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f62b7-e4be-4baf-8244-03f1b60e8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifiers(classifier, epochs, batch_size):\n",
    "    # Setup: Data, Error func, optimizer, NN classifier\n",
    "    # 1. Data\n",
    "    dataset = FpsDataset(fingerprints, labels) # initialize/load dataset obj\n",
    "    # creates a dataloader to batch data\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    # 2. Error\n",
    "    err_func = torch.nn.BCELoss()\n",
    "    # 3. Classifier\n",
    "    classifier = classifier\n",
    "    # 4. Optimizer\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), 0.001)\n",
    "    loss_list = [] # a plot of the error on training over time\n",
    "    # Training loop:\n",
    "    for i in range(epochs):\n",
    "        # loop through training batches\n",
    "        for idx, (data, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = torch.sigmoid(classifier(data))\n",
    "            error = err_func(predictions, y.reshape((y.size(0),1))) \n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "        # track error over whole dataset per epoch\n",
    "        predictions = torch.sigmoid(classifier(dataset.training_tens))\n",
    "        loss_list.append(err_func(predictions, dataset.tr_label_tens.reshape(\n",
    "            dataset.tr_label_tens.size(0),1)).detach().numpy())\n",
    "    # calc % error  \n",
    "    total_correct = 0\n",
    "    predictions = torch.sigmoid(classifier(dataset.test_tens))\n",
    "    test_predictions = predictions.reshape(-1).detach().numpy().astype(int)\n",
    "    test_labels = dataset.te_label_tens.detach().numpy()\n",
    "    for i in range(781):\n",
    "    if test_predictions[i] == test_labels[i]:\n",
    "        total_correct += 1\n",
    "    # return: classifier, error_list, and percent correct on test\n",
    "    return loss_list, total_correct/7.81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c43d7-eae1-4f31-9694-a2c3e2b5ac00",
   "metadata": {},
   "source": [
    "## Non-Lin Func tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cccdd1-c106-4b99-82fc-163411954a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_cl = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16,1)  \n",
    ")\n",
    "elu_cl = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.ELU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.ELU(),\n",
    "    torch.nn.Linear(32,16),\n",
    "    torch.nn.ELU(),\n",
    "    torch.nn.Linear(16,1)  \n",
    ")\n",
    "silu_cl = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(32,16),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(16,1)  \n",
    ")\n",
    "\n",
    "non_lin_list = [relu_cl,elu_cl,silu_cl]\n",
    "relu = []\n",
    "elu = []\n",
    "silu = []\n",
    "results = [relu, elu, silu]\n",
    "for i in range(10):\n",
    "    j = 0\n",
    "    for cl in non_lin_list:\n",
    "        results[j].append(test_classifiers(cl, 150, 32))\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08253456-2b03-47b8-ba68-ec3fc03cb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([1,2,3], [np.mean(relu), np.mean(elu), np.mean(silu)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f91ea-6b33-41ba-a9eb-4582c950b9bc",
   "metadata": {},
   "source": [
    "## Hidden Layer Number Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9c9f0-9516-4508-98dc-197c6c6dd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_cl = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(32,1), \n",
    ")\n",
    "four_cl = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(32,16),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(16,1)  \n",
    ")\n",
    "five_cl = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64,64),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(32,16),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(16,8),  \n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(8,1)\n",
    ")\n",
    "layer_list = [thr_cl,four_cl,five_cl]\n",
    "thr = []\n",
    "four = []\n",
    "five = []\n",
    "results = [thr, four, five]\n",
    "for i in range(10):\n",
    "    j = 0\n",
    "    for cl in layer_list:\n",
    "        results[j].append(test_classifiers(cl, 150, 32))\n",
    "        j+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
